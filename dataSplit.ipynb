{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Files saved in the respective folders.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define paths\n",
    "base_folder = \"data/roofNTNU/\"\n",
    "source_folder = os.path.join(base_folder, \"roof_clouds_normed/\")\n",
    "output_folder = os.path.join(base_folder, \"train_test_split/\")\n",
    "train_json = os.path.join(output_folder, \"shuffled_train_file_list.json\")\n",
    "test_json = os.path.join(output_folder, \"shuffled_test_file_list.json\")\n",
    "val_json = os.path.join(output_folder, \"shuffled_val_file_list.json\")\n",
    "\n",
    "# Create the required folder structure\n",
    "os.makedirs(os.path.join(output_folder, \"points_train_n\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_folder, \"labels_train_n\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_folder, \"points_test_n\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_folder, \"labels_test_n\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_folder, \"points_val_n\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_folder, \"labels_val_n\"), exist_ok=True)\n",
    "\n",
    "def process_files(file_list, points_folder, labels_folder):\n",
    "    \"\"\"\n",
    "    Process the files to extract points and labels and save them to the respective folders.\n",
    "    \"\"\"\n",
    "    for file_name in file_list:\n",
    "        input_file = os.path.join(source_folder, file_name+'.txt')\n",
    "        \n",
    "        # Output file paths\n",
    "        points_file = os.path.join(points_folder, file_name+'.txt')\n",
    "        labels_file = os.path.join(labels_folder, file_name+'.txt')\n",
    "        \n",
    "        with open(input_file, 'r') as infile, \\\n",
    "             open(points_file, 'w') as points_out, \\\n",
    "             open(labels_file, 'w') as labels_out:\n",
    "            \n",
    "            for line in infile:\n",
    "                values = line.strip().split()\n",
    "                if len(values) >= 4:  # Ensure the line has enough values\n",
    "                    # Write the first 3 values to the points file\n",
    "                    points_out.write(\" \".join(values[:3]) + \"\\n\")\n",
    "                    # Write the second last value to the labels file\n",
    "                    label = int(values[-2]) + 1\n",
    "                    labels_out.write(str(label) + \"\\n\")\n",
    "\n",
    "# Load the JSON files\n",
    "with open(train_json, 'r') as f:\n",
    "    train_files = json.load(f)\n",
    "\n",
    "with open(test_json, 'r') as f:\n",
    "    test_files = json.load(f)\n",
    "\n",
    "with open(val_json, 'r') as f:\n",
    "    val_files = json.load(f)\n",
    "\n",
    "# Process training files\n",
    "process_files(train_files, \n",
    "              os.path.join(output_folder, \"points_train_n\"), \n",
    "              os.path.join(output_folder, \"labels_train_n\"))\n",
    "\n",
    "# Process testing files\n",
    "process_files(test_files, \n",
    "              os.path.join(output_folder, \"points_test_n\"), \n",
    "              os.path.join(output_folder, \"labels_test_n\"))\n",
    "\n",
    "# Process validation files\n",
    "process_files(val_files,\n",
    "                os.path.join(output_folder, \"points_val_n\"), \n",
    "                os.path.join(output_folder, \"labels_val_n\"))\n",
    "\n",
    "print(\"Processing complete. Files saved in the respective folders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of .txt files: 1032\n",
      "Max lines in a file: 39172\n",
      "Min lines in a file: 31\n",
      "Average lines per file: 2177.60\n",
      "Median lines per file: 1789.0\n",
      "Standard deviation of lines: 2012.50\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the folder path containing the .xyz files\n",
    "folder_path = \"data/roofNTNU/roof_clouds_normed/\"  # Replace with your actual path\n",
    "\n",
    "# List all .xyz files in the folder\n",
    "xyz_files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
    "\n",
    "# Count the number of files\n",
    "num_files = len(xyz_files)\n",
    "\n",
    "# List to store the number of lines in each file\n",
    "line_counts = []\n",
    "\n",
    "# Read each file and count the lines\n",
    "for file_name in xyz_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    with open(file_path, 'r') as file:\n",
    "        num_lines = sum(1 for _ in file)  # Count lines\n",
    "        line_counts.append(num_lines)\n",
    "\n",
    "# Compute statistics\n",
    "max_lines = np.max(line_counts) if line_counts else 0\n",
    "min_lines = np.min(line_counts) if line_counts else 0\n",
    "avg_lines = np.mean(line_counts) if line_counts else 0\n",
    "median_lines = np.median(line_counts) if line_counts else 0\n",
    "std_dev_lines = np.std(line_counts) if line_counts else 0\n",
    "\n",
    "# Print summary results\n",
    "print(f\"Number of .txt files: {num_files}\")\n",
    "print(f\"Max lines in a file: {max_lines}\")\n",
    "print(f\"Min lines in a file: {min_lines}\")\n",
    "print(f\"Average lines per file: {avg_lines:.2f}\")\n",
    "print(f\"Median lines per file: {median_lines}\")\n",
    "print(f\"Standard deviation of lines: {std_dev_lines:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the folder path containing the .xyz files\n",
    "folder_path = \"data/roofNTNU/train_test_split/points_train_n\"  # Replace with your actual path\n",
    "\n",
    "# List all .xyz files in the folder\n",
    "xyz_files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
    "\n",
    "# Count the number of files\n",
    "num_files = len(xyz_files)\n",
    "\n",
    "# List to store the number of lines in each file\n",
    "line_counts = []\n",
    "\n",
    "# Read each file and count the lines\n",
    "for file_name in xyz_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    with open(file_path, 'r') as file:\n",
    "        num_lines = sum(1 for _ in file)  # Count lines\n",
    "        line_counts.append(num_lines)\n",
    "\n",
    "# Compute statistics\n",
    "max_lines = np.max(line_counts) if line_counts else 0\n",
    "min_lines = np.min(line_counts) if line_counts else 0\n",
    "avg_lines = np.mean(line_counts) if line_counts else 0\n",
    "median_lines = np.median(line_counts) if line_counts else 0\n",
    "std_dev_lines = np.std(line_counts) if line_counts else 0\n",
    "\n",
    "# Print summary results\n",
    "print(f\"Number of .txt files: {num_files}\")\n",
    "print(f\"Max lines in a file: {max_lines}\")\n",
    "print(f\"Min lines in a file: {min_lines}\")\n",
    "print(f\"Average lines per file: {avg_lines:.2f}\")\n",
    "print(f\"Median lines per file: {median_lines}\")\n",
    "print(f\"Standard deviation of lines: {std_dev_lines:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the folder path containing the .xyz files\n",
    "folder_path = \"data/roofNTNU/train_test_split/points_test_n\"  # Replace with your actual path\n",
    "\n",
    "# List all .xyz files in the folder\n",
    "xyz_files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
    "\n",
    "# Count the number of files\n",
    "num_files = len(xyz_files)\n",
    "\n",
    "# List to store the number of lines in each file\n",
    "line_counts = []\n",
    "\n",
    "# Read each file and count the lines\n",
    "for file_name in xyz_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    with open(file_path, 'r') as file:\n",
    "        num_lines = sum(1 for _ in file)  # Count lines\n",
    "        line_counts.append(num_lines)\n",
    "\n",
    "# Compute statistics\n",
    "max_lines = np.max(line_counts) if line_counts else 0\n",
    "min_lines = np.min(line_counts) if line_counts else 0\n",
    "avg_lines = np.mean(line_counts) if line_counts else 0\n",
    "median_lines = np.median(line_counts) if line_counts else 0\n",
    "std_dev_lines = np.std(line_counts) if line_counts else 0\n",
    "\n",
    "# Print summary results\n",
    "print(f\"Number of .txt files: {num_files}\")\n",
    "print(f\"Max lines in a file: {max_lines}\")\n",
    "print(f\"Min lines in a file: {min_lines}\")\n",
    "print(f\"Average lines per file: {avg_lines:.2f}\")\n",
    "print(f\"Median lines per file: {median_lines}\")\n",
    "print(f\"Standard deviation of lines: {std_dev_lines:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the folder path containing the .xyz files\n",
    "folder_path = \"data/roofNTNU/train_test_split/points_val_n\"  # Replace with your actual path\n",
    "\n",
    "# List all .xyz files in the folder\n",
    "xyz_files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
    "\n",
    "# Count the number of files\n",
    "num_files = len(xyz_files)\n",
    "\n",
    "# List to store the number of lines in each file\n",
    "line_counts = []\n",
    "\n",
    "# Read each file and count the lines\n",
    "for file_name in xyz_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    with open(file_path, 'r') as file:\n",
    "        num_lines = sum(1 for _ in file)  # Count lines\n",
    "        line_counts.append(num_lines)\n",
    "\n",
    "# Compute statistics\n",
    "max_lines = np.max(line_counts) if line_counts else 0\n",
    "min_lines = np.min(line_counts) if line_counts else 0\n",
    "avg_lines = np.mean(line_counts) if line_counts else 0\n",
    "median_lines = np.median(line_counts) if line_counts else 0\n",
    "std_dev_lines = np.std(line_counts) if line_counts else 0\n",
    "\n",
    "# Print summary results\n",
    "print(f\"Number of .txt files: {num_files}\")\n",
    "print(f\"Max lines in a file: {max_lines}\")\n",
    "print(f\"Min lines in a file: {min_lines}\")\n",
    "print(f\"Average lines per file: {avg_lines:.2f}\")\n",
    "print(f\"Median lines per file: {median_lines}\")\n",
    "print(f\"Standard deviation of lines: {std_dev_lines:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LiDARML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
