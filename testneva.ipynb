{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbff56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import classes from your main pipeline script\n",
    "try:\n",
    "    from trainModel import (\n",
    "        Config, \n",
    "        PointUNet, \n",
    "        LiDARPointCloudDataset, \n",
    "        cluster_embeddings\n",
    "    )\n",
    "except ImportError:\n",
    "    print(\"Error: Could not import from 'roof_segmentation_pipeline.py'.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390f9095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# HELPER FUNCTIONS\n",
    "# -------------------------------------------------------------------------\n",
    "def instance_mean_iou(gt_labels, pred_labels):\n",
    "    \"\"\"\n",
    "    Calculates mean IoU by finding the optimal matching between \n",
    "    ground truth and predicted instances using the Hungarian algorithm.\n",
    "    \"\"\"\n",
    "    gt_ids = np.unique(gt_labels)\n",
    "    pred_ids = np.unique(pred_labels)\n",
    "    \n",
    "    if len(gt_ids) == 0 or len(pred_ids) == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    iou_matrix = np.zeros((len(gt_ids), len(pred_ids)))\n",
    "\n",
    "    for i, gt_id in enumerate(gt_ids):\n",
    "        gt_mask = gt_labels == gt_id\n",
    "        for j, pred_id in enumerate(pred_ids):\n",
    "            pred_mask = pred_labels == pred_id\n",
    "            intersection = np.logical_and(gt_mask, pred_mask).sum()\n",
    "            union = np.logical_or(gt_mask, pred_mask).sum()\n",
    "            if union > 0:\n",
    "                iou_matrix[i, j] = intersection / union\n",
    "\n",
    "    # Maximize total IoU using linear sum assignment\n",
    "    row_ind, col_ind = linear_sum_assignment(-iou_matrix)\n",
    "    matched_ious = iou_matrix[row_ind, col_ind]\n",
    "    return matched_ious.mean()\n",
    "\n",
    "def save_cluster_plot(points, labels, path):\n",
    "    \"\"\"Saves a 3D scatter plot of the clusters.\"\"\"\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    # Using tab20 for distinct colors\n",
    "    ax.scatter(points[:, 0], points[:, 1], points[:, 2], c=labels, cmap='tab20', s=2)\n",
    "    ax.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d9caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(conf: Config = None, model_path=\"roof_segmentation_dgcnn_best.pth\", vis_output_dir=\"./evaluation_results\"):\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1. SETUP\n",
    "    # -------------------------------------------------------------------------\n",
    "    if conf is None:\n",
    "        conf = Config()\n",
    "    # MODEL_PATH = \"roof_segmentation_dgcnn_best.pth\"\n",
    "    # VIS_OUTPUT_DIR = \"./evaluation_results\"\n",
    "    os.makedirs(vis_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Use HDBSCAN if available (from pipeline config), else MeanShift\n",
    "    clustering_method = conf.clustering_method\n",
    "\n",
    "    print(f\"--- Evaluation Settings ---\")\n",
    "    print(f\"Model: {model_path}\")\n",
    "    print(f\"Clustering: {clustering_method}\")\n",
    "    print(f\"Data Split: TEST\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2. LOAD DATA\n",
    "    # -------------------------------------------------------------------------\n",
    "    test_dataset = LiDARPointCloudDataset(\n",
    "        base_dir=\"data/roofNTNU/train_test_split\", \n",
    "        split='test', \n",
    "        max_points=conf.max_points,\n",
    "        sampling_method=conf.sampling_method\n",
    "    )\n",
    "    \n",
    "    if len(test_dataset) == 0:\n",
    "        print(\"No test data found. Please check data/train_test_split/points_test_n\")\n",
    "        return\n",
    "\n",
    "    # Batch size 1 for accurate per-instance metric calculation\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3. LOAD MODEL\n",
    "    # -------------------------------------------------------------------------\n",
    "    model = PointUNet(conf).to(conf.device)\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Checkpoint not found at {model_path}. Cannot evaluate.\")\n",
    "        return\n",
    "        \n",
    "    checkpoint = torch.load(model_path, map_location=conf.device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4. EVALUATION LOOP\n",
    "    # -------------------------------------------------------------------------\n",
    "    all_metrics = []\n",
    "    complexity_buckets = defaultdict(list) # key = complexity, value = list of (ARI, mIoU)\n",
    "    \n",
    "    print(\"Starting Inference...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (points, labels, _) in enumerate(tqdm(test_loader)):\n",
    "            scene_name = f\"scene_{i:03d}\"\n",
    "            \n",
    "            points = points.to(conf.device)\n",
    "            gt_labels_full = labels.cpu().numpy()[0]\n",
    "            \n",
    "            # Forward Pass\n",
    "            embeddings = model(points)\n",
    "            emb_sample = embeddings[0].cpu().numpy()\n",
    "            \n",
    "            # Clustering\n",
    "            pred_labels_full = cluster_embeddings(emb_sample, method=clustering_method)\n",
    "            \n",
    "            # -----------------------------------------------------------\n",
    "            # 5. METRICS & COMPLEXITY\n",
    "            # -----------------------------------------------------------\n",
    "            # Filter padding (-1)\n",
    "            valid_mask = gt_labels_full != -1\n",
    "            if valid_mask.sum() == 0: continue\n",
    "                \n",
    "            gt_valid = gt_labels_full[valid_mask]\n",
    "            pred_valid = pred_labels_full[valid_mask]\n",
    "            points_valid = points[0].cpu().numpy()[valid_mask] # For visualization\n",
    "            \n",
    "            # Calculate Metrics\n",
    "            ari = adjusted_rand_score(gt_valid, pred_valid)\n",
    "            nmi = normalized_mutual_info_score(gt_valid, pred_valid)\n",
    "            miou = instance_mean_iou(gt_valid, pred_valid)\n",
    "            \n",
    "            # Determine Complexity\n",
    "            num_gt_instances = len(np.unique(gt_valid))\n",
    "            if num_gt_instances <= 2:\n",
    "                complexity = \"simple\"\n",
    "            elif num_gt_instances <= 5:\n",
    "                complexity = \"moderate\"\n",
    "            else:\n",
    "                complexity = \"complex\"\n",
    "                \n",
    "            # Store results\n",
    "            result_entry = {\n",
    "                \"scene\": scene_name,\n",
    "                \"ARI\": float(ari),\n",
    "                \"NMI\": float(nmi),\n",
    "                \"mIoU\": float(miou),\n",
    "                \"gt_instances\": int(num_gt_instances),\n",
    "                \"pred_instances\": int(len(np.unique(pred_valid))),\n",
    "                \"complexity\": complexity\n",
    "            }\n",
    "            all_metrics.append(result_entry)\n",
    "            complexity_buckets[complexity].append((ari, miou))\n",
    "            \n",
    "            # -----------------------------------------------------------\n",
    "            # 6. VISUALIZATION\n",
    "            # -----------------------------------------------------------\n",
    "            if i < 10: # Save first 10\n",
    "                vis_path = os.path.join(vis_output_dir, f\"{scene_name}_pred.png\")\n",
    "                save_cluster_plot(points_valid, pred_valid, vis_path)\n",
    "                \n",
    "                # Optional: Save .txt for CloudCompare\n",
    "                txt_path = os.path.join(vis_output_dir, f\"{scene_name}.txt\")\n",
    "                save_data = np.column_stack((points_valid, pred_valid, gt_valid))\n",
    "                np.savetxt(txt_path, save_data, fmt=\"%.6f %.6f %.6f %d %d\", header=\"x y z pred gt\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7. SUMMARY REPORT\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Save JSON\n",
    "    with open(os.path.join(vis_output_dir, \"evaluation_summary.json\"), \"w\") as f:\n",
    "        json.dump(all_metrics, f, indent=2)\n",
    "\n",
    "    avg_ari = np.mean([m['ARI'] for m in all_metrics]) if all_metrics else 0.0\n",
    "    avg_miou = np.mean([m['mIoU'] for m in all_metrics]) if all_metrics else 0.0\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"           EVALUATION REPORT           \")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total Samples: {len(all_metrics)}\")\n",
    "    print(f\"Overall Mean ARI:  {avg_ari:.4f}\")\n",
    "    print(f\"Overall Mean mIoU: {avg_miou:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Performance by Roof Complexity:\")\n",
    "    \n",
    "    for level in [\"simple\", \"moderate\", \"complex\"]:\n",
    "        if level in complexity_buckets:\n",
    "            scores = complexity_buckets[level]\n",
    "            aris = [a for a, _ in scores]\n",
    "            ious = [m for _, m in scores]\n",
    "            print(f\"  {level.capitalize().ljust(10)} ({len(scores)} scenes): ARI={np.mean(aris):.3f}, mIoU={np.mean(ious):.3f}\")\n",
    "        else:\n",
    "            print(f\"  {level.capitalize().ljust(10)} (0 scenes): N/A\")\n",
    "            \n",
    "    print(\"=\"*50)\n",
    "    print(f\"Results saved to: {vis_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58684376",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LiDARML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
